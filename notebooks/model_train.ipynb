{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5 Training on COCO128\n",
    "\n",
    "This notebook contains the complete training pipeline for YOLOv5 on the COCO128 dataset.\n",
    "\n",
    "## M1 Metal (MPS) Acceleration\n",
    "\n",
    "This notebook is configured to use Apple's Metal Performance Shaders (MPS) backend for GPU acceleration on M1/M2 Macs.\n",
    "\n",
    "**Requirements:**\n",
    "- PyTorch >= 1.12.0 (MPS support)\n",
    "- macOS 12.3+ (for MPS)\n",
    "\n",
    "The notebook will automatically detect and use:\n",
    "1. **MPS** (Metal) if available (Apple Silicon)\n",
    "2. **CUDA** if available (NVIDIA GPU)\n",
    "3. **CPU** as fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "parent_dir = Path.cwd().parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from minimal_yolov5 import *\n",
    "from train_yolov5 import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch and MPS availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"\\n✓ MPS (Metal) acceleration is available and will be used!\")\n",
    "    print(\"  Your M1 GPU will accelerate training significantly.\")\n",
    "else:\n",
    "    print(\"\\n✗ MPS not available. Will use CPU.\")\n",
    "    print(\"  To enable MPS, upgrade PyTorch: pip install --upgrade torch torchvision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths for COCO128 dataset\n",
    "dataset_root = Path(\"/Users/davide/Documents/Learning/Datasets/coco128\")\n",
    "img_dir = dataset_root / \"images\" / \"train2017\"\n",
    "label_dir = dataset_root / \"labels\" / \"train2017\"\n",
    "\n",
    "print(f\"Dataset root: {dataset_root}\")\n",
    "print(f\"Images dir: {img_dir}\")\n",
    "print(f\"Labels dir: {label_dir}\")\n",
    "print(f\"\\nImages exist: {img_dir.exists()}\")\n",
    "print(f\"Labels exist: {label_dir.exists()}\")\n",
    "\n",
    "# Count files\n",
    "if img_dir.exists():\n",
    "    num_images = len(list(img_dir.glob(\"*.jpg\")))\n",
    "    print(f\"\\nNumber of images: {num_images}\")\n",
    "if label_dir.exists():\n",
    "    num_labels = len(list(label_dir.glob(\"*.txt\")))\n",
    "    print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "print(\"Creating COCO128 dataset...\")\n",
    "dataset = YOLODataset(\n",
    "    img_dir=str(img_dir),\n",
    "    label_dir=str(label_dir),\n",
    "    img_size=640,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Split into train/val (80/20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_dataset)} images\")\n",
    "print(f\"Val set: {len(val_dataset)} images\")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0  # Set to 0 for notebook\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading one batch\n",
    "print(\"Loading a sample batch...\")\n",
    "imgs, targets = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nBatch images shape: {imgs.shape}\")\n",
    "print(f\"Batch targets shape: {targets.shape}\")\n",
    "print(f\"Number of objects in batch: {len(targets)}\")\n",
    "\n",
    "# Show some sample labels\n",
    "print(f\"\\nFirst 5 targets (img_idx, class, x, y, w, h):\")\n",
    "print(targets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training with M1 Metal (MPS) support\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: MPS (Apple M1 Metal)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: CPU\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating model...\")\n",
    "model = YOLOv5(num_classes=80, channels=3)\n",
    "model = model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=100\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = YOLOLoss(model)\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_box_loss = 0\n",
    "    train_obj_loss = 0\n",
    "    train_cls_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for imgs, targets in pbar:\n",
    "        imgs = imgs.to(device).float() / 255.0\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(imgs)\n",
    "        loss, loss_items = loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track losses\n",
    "        train_loss += loss.item()\n",
    "        train_box_loss += loss_items[0].item()\n",
    "        train_obj_loss += loss_items[1].item()\n",
    "        train_cls_loss += loss_items[2].item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'box': f'{loss_items[0]:.4f}',\n",
    "            'obj': f'{loss_items[1]:.4f}',\n",
    "            'cls': f'{loss_items[2]:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Average training losses\n",
    "    train_loss /= len(train_loader)\n",
    "    train_box_loss /= len(train_loader)\n",
    "    train_obj_loss /= len(train_loader)\n",
    "    train_cls_loss /= len(train_loader)\n",
    "    \n",
    "    # ========== Validation ==========\n",
    "    # NOTE: We keep model in training mode because the loss function\n",
    "    # expects predictions in training format (list of tensors), not inference format\n",
    "    val_loss = 0\n",
    "    val_box_loss = 0\n",
    "    val_obj_loss = 0\n",
    "    val_cls_loss = 0\n",
    "    \n",
    "    # Keep model in train mode but disable gradients\n",
    "    model.train()\n",
    "    \n",
    "    for imgs, targets in tqdm(val_loader, desc='Validation'):\n",
    "        imgs = imgs.to(device).float() / 255.0\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Use torch.no_grad() to disable gradient computation\n",
    "        with torch.no_grad():\n",
    "            predictions = model(imgs)\n",
    "            loss, loss_items = loss_fn(predictions, targets)\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        val_box_loss += loss_items[0].item()\n",
    "        val_obj_loss += loss_items[1].item()\n",
    "        val_cls_loss += loss_items[2].item()\n",
    "    \n",
    "    # Average validation losses\n",
    "    val_loss /= len(val_loader)\n",
    "    val_box_loss /= len(val_loader)\n",
    "    val_obj_loss /= len(val_loader)\n",
    "    val_cls_loss /= len(val_loader)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} (box: {train_box_loss:.4f}, obj: {train_obj_loss:.4f}, cls: {train_cls_loss:.4f})\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} (box: {val_box_loss:.4f}, obj: {val_obj_loss:.4f}, cls: {val_cls_loss:.4f})\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot loss difference\n",
    "plt.subplot(1, 2, 2)\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "diff = [v - t for t, v in zip(history['train_loss'], history['val_loss'])]\n",
    "plt.plot(epochs, diff, marker='o', color='purple')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Val Loss - Train Loss')\n",
    "plt.title('Overfitting Monitor')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COCO Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO dataset class names (80 classes)\n",
    "COCO_CLASSES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
    "    'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "print(f\"COCO dataset has {len(COCO_CLASSES)} classes\")\n",
    "print(f\"First 10 classes: {COCO_CLASSES[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Debug Prediction Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check prediction statistics\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get a batch from validation set\n",
    "    imgs, targets = next(iter(val_loader))\n",
    "    imgs = imgs.to(device).float() / 255.0\n",
    "    \n",
    "    # Get predictions\n",
    "    pred = model(imgs)[0]  # Shape: (N, 85) where N = batch_size * 25200\n",
    "    \n",
    "    print(f\"Predictions shape: {pred.shape}\")\n",
    "    print(f\"\\nObjectness confidence statistics:\")\n",
    "    print(f\"  Min: {pred[:, 4].min().item():.6f}\")\n",
    "    print(f\"  Max: {pred[:, 4].max().item():.6f}\")\n",
    "    print(f\"  Mean: {pred[:, 4].mean().item():.6f}\")\n",
    "    print(f\"  Median: {pred[:, 4].median().item():.6f}\")\n",
    "    \n",
    "    # Count predictions above different thresholds\n",
    "    for thresh in [0.01, 0.05, 0.1, 0.3, 0.5]:\n",
    "        count = (pred[:, 4] > thresh).sum().item()\n",
    "        print(f\"  Predictions > {thresh}: {count}\")\n",
    "    \n",
    "    print(f\"\\nNote: If max confidence is very low (< 0.01), the model needs more training.\")\n",
    "    print(f\"After proper training, you should see max confidence > 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set - showing original images with properly transformed boxes\n",
    "model.eval()\n",
    "\n",
    "# Get random samples from validation set\n",
    "num_samples = 4\n",
    "indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "conf_thresh = 0.05  # Lower threshold for visualization\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, ax in zip(indices, axes):\n",
    "        # Get the actual dataset item\n",
    "        dataset_idx = val_dataset.indices[idx]\n",
    "        img_path = dataset.img_files[dataset_idx]\n",
    "        \n",
    "        # Load ORIGINAL image (before letterbox)\n",
    "        orig_img = cv2.imread(str(img_path))\n",
    "        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "        h_orig, w_orig = orig_img.shape[:2]\n",
    "        \n",
    "        # Load ORIGINAL labels (before letterbox adjustment)\n",
    "        label_path = dataset.label_dir / (img_path.stem + '.txt')\n",
    "        orig_labels = []\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    orig_labels.append([cls, x, y, w, h])\n",
    "        orig_labels = np.array(orig_labels) if orig_labels else np.zeros((0, 5))\n",
    "        \n",
    "        # Get letterboxed image and predictions\n",
    "        img, _ = val_dataset[idx]\n",
    "        img_tensor = img.unsqueeze(0).to(device).float() / 255.0\n",
    "        pred = model(img_tensor)[0]\n",
    "        \n",
    "        # Filter predictions\n",
    "        pred = pred[pred[:, 4] > conf_thresh]\n",
    "        if len(pred) > 0:\n",
    "            pred = pred[pred[:, 4].argsort(descending=True)][:20]\n",
    "        \n",
    "        # Display ORIGINAL image\n",
    "        ax.imshow(orig_img)\n",
    "        \n",
    "        # Draw ground truth boxes in green (on original image)\n",
    "        for label in orig_labels:\n",
    "            if len(label) == 0:\n",
    "                continue\n",
    "            cls, x_norm, y_norm, w_norm, h_norm = label\n",
    "            \n",
    "            # Convert from normalized to pixel coordinates on ORIGINAL image\n",
    "            x_center = x_norm * w_orig\n",
    "            y_center = y_norm * h_orig\n",
    "            box_w = w_norm * w_orig\n",
    "            box_h = h_norm * h_orig\n",
    "            \n",
    "            x1 = x_center - box_w / 2\n",
    "            y1 = y_center - box_h / 2\n",
    "            \n",
    "            rect = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, \n",
    "                                    edgecolor='green', facecolor='none', linestyle='-')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Get class name\n",
    "            class_idx = int(cls)\n",
    "            class_name = COCO_CLASSES[class_idx] if class_idx < len(COCO_CLASSES) else f'C{class_idx}'\n",
    "            \n",
    "            ax.text(x1, max(10, y1 - 5), f'GT: {class_name}', \n",
    "                   color='white', fontsize=8, weight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='green', alpha=0.8, edgecolor='none'))\n",
    "        \n",
    "        # Transform predictions back from letterbox to original coordinates\n",
    "        # Calculate letterbox parameters\n",
    "        r = min(640 / h_orig, 640 / w_orig)\n",
    "        new_unpad = (int(round(w_orig * r)), int(round(h_orig * r)))\n",
    "        dw = (640 - new_unpad[0]) / 2\n",
    "        dh = (640 - new_unpad[1]) / 2\n",
    "        \n",
    "        # Draw predictions in red (mapped back to original image)\n",
    "        for p in pred:\n",
    "            # Predictions are in letterbox coordinates (640x640)\n",
    "            x_center, y_center, box_w, box_h = p[:4].cpu().numpy()\n",
    "            \n",
    "            # Remove letterbox padding\n",
    "            x_center_unpad = x_center - dw\n",
    "            y_center_unpad = y_center - dh\n",
    "            \n",
    "            # Scale back to original image size\n",
    "            x_center_orig = x_center_unpad / r\n",
    "            y_center_orig = y_center_unpad / r\n",
    "            box_w_orig = box_w / r\n",
    "            box_h_orig = box_h / r\n",
    "            \n",
    "            x1 = x_center_orig - box_w_orig / 2\n",
    "            y1 = y_center_orig - box_h_orig / 2\n",
    "            \n",
    "            # Only draw if box is within original image bounds\n",
    "            if (x1 >= 0 and y1 >= 0 and \n",
    "                x1 + box_w_orig <= w_orig and \n",
    "                y1 + box_h_orig <= h_orig and\n",
    "                box_w_orig > 5 and box_h_orig > 5):\n",
    "                \n",
    "                rect = patches.Rectangle((x1, y1), box_w_orig, box_h_orig, linewidth=2, \n",
    "                                        edgecolor='red', facecolor='none', linestyle='-')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                conf = p[4].item()\n",
    "                cls_idx = p[5:].argmax().item()\n",
    "                class_name = COCO_CLASSES[cls_idx] if cls_idx < len(COCO_CLASSES) else f'C{cls_idx}'\n",
    "                \n",
    "                ax.text(x1, max(10, y1 - 5), f'{class_name}: {conf:.2f}', \n",
    "                       color='white', fontsize=8, weight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.8, edgecolor='none'))\n",
    "        \n",
    "        num_preds = len(pred) if len(pred) > 0 else 0\n",
    "        ax.set_title(f'Image {dataset_idx} - Green: GT ({len(orig_labels)} objs), Red: {num_preds} Preds', \n",
    "                     fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nShowing predictions with confidence > {conf_thresh}\")\n",
    "print(\"Green boxes: Ground truth (on original image)\")\n",
    "print(\"Red boxes: Model predictions (transformed back from letterbox)\")\n",
    "print(f\"\\nNote: Original images shown without letterbox padding for clarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = Path(\"../checkpoints\")\n",
    "save_path.mkdir(exist_ok=True)\n",
    "\n",
    "checkpoint_path = save_path / \"yolov5_coco128_100epochs.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': history['train_loss'][-1],\n",
    "    'val_loss': history['val_loss'][-1],\n",
    "    'history': history\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"Model saved to: {checkpoint_path}\")\n",
    "print(f\"Checkpoint includes: model weights, optimizer state, and training history\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
